+++
title="hadoop 实战（一）| hadoop 安装"
tags=["hadoop"]
categories=["hadoop"]
date="2020-04-21T21:00:00+08:00"
+++
## 1.环境准备
### 1.1 服务器规划
```
# -u root -p centosdata 
172.16.232.130 centos701
172.16.232.131 centos702
172.16.232.132 centos703

```
### 1.2 关闭所有服务器的防火墙
```
systemctl stop firewalld.service # 停止firewall。
systemctl disable firewalld.service # 禁止firewall开机启动。
firewall-cmd --state # 查看默认防火墙装状态(关闭后显示notrunning, 开启显示running)。
```
### 1.3 关闭服务器的SLNEX
```
vim /etc/selinux/config
SELINUX=disabled
```
### 1.4 服务器ssh 安装
- 所有服务器相互互通。 

## 2. 配置hadoop环境变量
### 2.1 hadoop解压后文件组织结构
- 下载地址：http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz
- 
```
├── bin
├── etc
├── include
├── lib
├── libexec
├── LICENSE.txt
├── logs
├── NOTICE.txt
├── README.txt
├── sbin
└── share
```
### 2.2 配置环境变量
```
# hadoop
export HADOOP_HOME=/opt/hadoop-2.7.7
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_COMMON_LIB_NATIVE_DIR"
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

## 3.配置hadoop-env.sh、mapred-env.sh、yarn-env.sh
### 3.1 配置hadoop-env.sh
```
export JAVA_HOME=${JAVA_HOME} 修改成:
export JAVA_HOME=/opt/jdk
```
### 3.2 配置core-site.xml
- 设置 Hadoop 的临时目录和文件系统，localhost:9000 表示本地主机。在 core-site.xml 文件里作如下配置
```
<configuration>
   <property>
        <name>hadoop.tmp.dir</name>
        <value>/root/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
   </property>
   <property>
        <name>fs.default.name</name>
        <value>hdfs://centos701:9000</value>
   </property>
</configuration>
```
### 3.3 配置hdfs-site.xml
- hdfs-site.xml 的配置修改如下，注意 name 和 data 的路径都要替换成本地的路径：
```
   <property>
     <name>dfs.name.dir</name>
     <value>/root/hadoop/dfs/name</value>
     <description>Path on the local filesystem where theNameNode stores the namespace and transactions logs persistently.</description>
   </property>
   <property>
     <name>dfs.data.dir</name>
     <value>/root/hadoop/dfs/data</value>
     <description>Comma separated list of paths on the localfilesystem of a DataNode where it should store its blocks.</description>
   </property>
   <property>
     <name>dfs.replication</name>
     <value>2</value>
   </property>
   <property>
     <name>dfs.permissions</name>
     <value>false</value>
     <description>need not permissions</description>
</property>
```
### 3.4mapred-site.xml 文件
```
 <property>
     <name>mapred.job.tracker</name>
     <value>centos701:49001</value>
  </property>
  <property>
     <name>mapred.local.dir</name>
     <value>/root/hadoop/var</value>
  </property>
  <property>
     <name>mapreduce.framework.name</name>
     <value>yarn</value>
  </property>
```
### 3.5yarn-site.xml 文件
```
<!-- Site specific YARN configuration properties -->
  <property>
       <name>yarn.resourcemanager.hostname</name>
       <value>centos701</value>
  </property>
  <property>
       <description>The address of the applications manager interface in the RM.</description>
       <name>yarn.resourcemanager.address</name>
       <value>${yarn.resourcemanager.hostname}:8032</value>
   </property>
   <property>
        <description>The address of the scheduler interface.</description>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>${yarn.resourcemanager.hostname}:8030</value>
   </property>
   <property>
        <description>The http address of the RM web application.</description>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>${yarn.resourcemanager.hostname}:8088</value>
   </property>
   <property>
        <description>The https adddress of the RM web application.</description>
        <name>yarn.resourcemanager.webapp.https.address</name>
        <value>${yarn.resourcemanager.hostname}:8090</value>
   </property>
   <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>${yarn.resourcemanager.hostname}:8031</value>
   </property>
   <property>
        <description>The address of the RM admin interface.</description>
        <name>yarn.resourcemanager.admin.address</name>
        <value>${yarn.resourcemanager.hostname}:8033</value>
   </property>
   <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
   </property>
   <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
        <discription>每个节点可用内存,单位MB,默认8182MB</discription>
   </property>
   <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
      	<value>2.1</value>
   </property>
   <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2048</value>
   </property>
   <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
   </property>
```

### 3.6 slaves
```
192.168.0.183
192.168.0.184
```
## 4.启动hadoopp
```
(1)初始化，输入命令，bin/hdfs namenode -format
(2)全部启动sbin/start-all.sh
(3)停止的话，输入命令，sbin/stop-all.sh
(4)输入命令，jps，可以看到相关信息
```

## 5.访问hadoop
```
(1)输入命令，systemctl stop firewalld.service
(2)浏览器打开http://172.16.232.130:8088/
(3)浏览器打开http://172.16.232.130:50070/
```

## 6.使用hadoop

### 6.1 HDFS 中创建用户目录：
- hadoop fs
- hadoop dfs
- hdfs dfs
```
1. hadoop fs适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统
2. hadoop dfs只能适用于HDFS文件系统
3. hdfs dfs跟hadoop dfs的命令作用一样，也只能适用于HDFS文件系统
```
```
> hdfs dfs -mkdir -p /user/hadoop
> 
```